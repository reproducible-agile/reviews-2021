---
title: "Reproducibility review of: A Comparative Study of Typing and Speech For Map Metadata Creation"
author: "F.O. Ostermann \\orcid{0000-0002-9317-8291} and Daniel Nüst \\orcid{0000-0002-0024-5046}"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  pdf_document:
    toc: false
papersize: a4
header-includes:
  - |
    % https://tex.stackexchange.com/questions/445563/ieeetran-how-to-include-orcid-in-tex-pdf-with-pdflatex/445583 (works with pdflatex)
    \usepackage{scalerel}
    \usepackage{tikz}
    \usetikzlibrary{svg.path}
    \definecolor{orcidlogocol}{HTML}{A6CE39}
    \tikzset{
      orcidlogo/.pic={
        \fill[orcidlogocol] svg{M256,128c0,70.7-57.3,128-128,128C57.3,256,0,198.7,0,128C0,57.3,57.3,0,128,0C198.7,0,256,57.3,256,128z};
        \fill[white] svg{M86.3,186.2H70.9V79.1h15.4v48.4V186.2z}
                     svg{M108.9,79.1h41.6c39.6,0,57,28.3,57,53.6c0,27.5-21.5,53.6-56.8,53.6h-41.8V79.1z     M124.3,172.4h24.5c34.9,0,42.9-26.5,42.9-39.7c0-21.5-13.7-39.7-43.7-39.7h-23.7V172.4z}
                     svg{M88.7,56.8c0,5.5-4.5,10.1-10.1,10.1c-5.6,0-10.1-4.6-10.1-10.1c0-5.6,4.5-10.1,10.1-10.1C84.2,46.7,88.7,51.3,88.7,56.8z};
      }
    }
    \newcommand\orcid[1]{\href{https://orcid.org/#1}{\raisebox{0.15 em}{\mbox{\scalerel*{
    \begin{tikzpicture}[yscale=-1, transform shape]
    \pic{orcidlogo};
    \end{tikzpicture}
    }{|}}}}}
    \definecolor{agileblue}{RGB}{0,77,155}
urlcolor: agileblue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

```{r logo, eval=TRUE, echo=FALSE, message=FALSE, fig.align='center', out.width='0.3\\linewidth', fig.pos='H'}
temp <- tempfile(fileext = ".pdf")
download.file(url = "https://reproducible-agile.github.io/public/images/reproducible-AGILE-logo-square.pdf", destfile = temp)
knitr::include_graphics(temp)
```

This report is part of the reproducibility review at the AGILE conference.
For more information see [https://reproducible-agile.github.io/](https://reproducible-agile.github.io/).
This document is published on OSF at <https://osf.io/7fqtm/>.
To cite the report use

> Ostermann, F. O., & Nüst, D. (2021, May 3). Reproducibility review of: A Comparative Study of Typing and Speech For Map Metadata Creation. https://doi.org/10.17605/osf.io/7fqtm

# Reviewed paper

>  Lai, P.-C. and Degbelo, A.: A Comparative Study of Typing and Speech For Map Metadata Creation, AGILE GIScience Ser., 2, 7, <https://doi.org/10.5194/agile-giss-2-7-2021>, 2021.

# Summary

The paper presents the results of a user experiment to improve GI-metadata using speech. A complete reproduction is practically impossible to achieve.
This reproducibility report therefore investigated two components:
First, whether sufficient information is provided to replicate the experiment elsewhere with a different group of participants.
Second, whether sufficient information is provided to reproduce the analysis of the experimental results.
The conclusion is positive for both:
To replicate the experiment, detailed information on participants is given.
Further, a Github repository (https://github.com/duckravel/Metadata) contains the source code of the prototype that was implemented for the user experiment, and the original prototype is accessible (https://enigmatic-basin-78677.herokuapp.com/#/welcome) at the time of this writing.
To reproduce the results, the provided input data, R code, and Excel spreadsheet (https://figshare.com/s/4c62ca5bf468d7be6e7d) lead to the same results as given in the paper and the prototype for the user experiment could be run locally.

\clearpage

# Reproducibility reviewer notes

This review focuses on the reproduction of the analysis results.
No in-depth examination of the prototype's code was conducted, but it was confirmed that the provided code and be run and seems to provide the application used in the study. Using 

```bash
# with npm version 6.14.8 and node version 14.13.0
npm install
npm start
```

we could run the application on http://localhost:8080, as shown in the screenshot below.

```{r screenshot, eval=TRUE, echo=FALSE, message=FALSE, fig.align='center', out.width='0.5\\linewidth', fig.cap="Screenshot of application executed locally"}
knitr::include_graphics("screenshot-map-annotation-app.jpg")
```

A clear license is missing in the repository.
The most important information (software architecture overview, exact questionnaire, maps used in the experiment) is also provided as supplementary material (PDF) on Figshare.
What is missing is the actual output of the participants, i.e., the annotations and metadata they produced.
This not an an impediment to reproduce this paper, because the content was not analyzed in depth (some examples are given).
For a paper that would analyze the content of the produced metadata, this would have to be provided.   

The input data and code for the analysis of the experimental results is provided [on Figshare](https://figshare.com/s/4c62ca5bf468d7be6e7d) in a Zip-Archive, which contains a short readme-file and five folders that are named according to the tables in the paper where the outputs are presented. 

Each folder contains a CSV input file with header and a short R-Script that performs descriptive statistical analysis on the input data, but does not produce the tables in the paper directly.
The script logic is simple to follow.
One line of code needs manual adjustment to point to the correct path for the input data.
This is obvious, but also explained in the readme-file. 

All scripts ran without error at negligible runtime and produce the results presented in the paper.
The most demanding task for the reviewer was to link the results from the scripts with the content in the tables, since neither variable names nor layout (scripts don't produce tables) match those in the paper.
Reliable matching was possible through simple comparison of outputs and paper content. 

One folder contained Excel spreadsheets which apparently are from an external source and contain several tabs with statistical analysis of the user experience questionnaire.
No source is given except an author name on the first tab.

The initial version of the paper had a Data and Software Availability section, but only anonymized links to Github and Figshare.
The authors provided these immediately upon request.
Overall, the authors did a commendable job at making a paper on user experiments as reproducible as possible. The following recommendations for improvement are desirable:

1. data on participants (age, gender, experience with maps, etc.) are "hidden" in the input CSV files - an explicit file with that information would be convenient
2. ~~the link between the variable names in the script and the tables in the paper is not always immediately clear, but can be inferred~~ README was later updated by authors
3. the origin of the Excel tables need to be clarified - where do these come from? an author is given, but that is incomplete
4. ~~the repository needs a license file for both the software and the data~~ partly resolved through <https://github.com/duckravel/Metadata/commit/2da3036c680b370e13578429673c80eb26c9dec3> (Apache is not a suitable license for _data_)
5. ~~the code and data repository should be properly archived (e.g., on Zenodo [with the GitHub-Zenodo-Integration](https://guides.github.com/activities/citable-code/)) and own code/data cited in the paper with a DOI~~ resolved through <https://doi.org/10.5281/zenodo.4739991>
6. ~~after acceptance, the Figshare record should be made public and missing metadata (authors etc.) should be added~~ - authors did this, see <https://doi.org/10.6084/m9.figshare.14207735.v2>
7. ~~use the [`here`](https://here.r-lib.org/) package in R to resolve file names, it should not be required to fix paths manually~~ - seems resolved through relative file paths in <https://doi.org/10.6084/m9.figshare.14207735.v3> (not tested!)

```{r, echo=FALSE, eval=FALSE, results='hide'}
library("osfr") # See docs at https://docs.ropensci.org/osfr/
# OSF_PAT is in .Renviron in parent directory
# We cannot use osfr to create a new component (with osfr::osf_create_component(x = osfr::osf_retrieve_node("6k5fh"), ...) because that will set the storage location to outside Europe.

# retrieve project
project <- osfr::osf_retrieve_node("7fqtm")

# upload files
osfr::osf_upload(x = project,
                 conflicts = "overwrite",
                 path = c(list.files(here::here("015"),
                                     pattern = "reproreview-agile-.*(pdf$|Rmd$|zip$)",
                                     full.names = TRUE)
                          )
                 )
```
