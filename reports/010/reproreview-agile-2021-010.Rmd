---
title: "Reproducibility review of: Extraction of linear structures from digital terrain models using deep learning"
author: "Daniel Nüst \\orcid{0000-0002-0024-5046} and Anita Graser \\orcid{0000-0001-5361-2885}"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  pdf_document:
    toc: false
papersize: a4
header-includes:
  - |
    % https://tex.stackexchange.com/questions/445563/ieeetran-how-to-include-orcid-in-tex-pdf-with-pdflatex/445583 (works with pdflatex)
    \usepackage{scalerel}
    \usepackage{tikz}
    \usetikzlibrary{svg.path}
    \definecolor{orcidlogocol}{HTML}{A6CE39}
    \tikzset{
      orcidlogo/.pic={
        \fill[orcidlogocol] svg{M256,128c0,70.7-57.3,128-128,128C57.3,256,0,198.7,0,128C0,57.3,57.3,0,128,0C198.7,0,256,57.3,256,128z};
        \fill[white] svg{M86.3,186.2H70.9V79.1h15.4v48.4V186.2z}
                     svg{M108.9,79.1h41.6c39.6,0,57,28.3,57,53.6c0,27.5-21.5,53.6-56.8,53.6h-41.8V79.1z     M124.3,172.4h24.5c34.9,0,42.9-26.5,42.9-39.7c0-21.5-13.7-39.7-43.7-39.7h-23.7V172.4z}
                     svg{M88.7,56.8c0,5.5-4.5,10.1-10.1,10.1c-5.6,0-10.1-4.6-10.1-10.1c0-5.6,4.5-10.1,10.1-10.1C84.2,46.7,88.7,51.3,88.7,56.8z};
      }
    }
    \newcommand\orcid[1]{\href{https://orcid.org/#1}{\raisebox{0.15 em}{\mbox{\scalerel*{
    \begin{tikzpicture}[yscale=-1, transform shape]
    \pic{orcidlogo};
    \end{tikzpicture}
    }{|}}}}}
    \definecolor{agileblue}{RGB}{0,77,155}
urlcolor: agileblue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

```{r logo, eval=TRUE, echo=FALSE, message=FALSE, fig.align='center', out.width='0.3\\linewidth', fig.pos='H'}
temp <- tempfile(fileext = ".pdf")
download.file(url = "https://reproducible-agile.github.io/public/images/reproducible-AGILE-logo-square.pdf", destfile = temp)
knitr::include_graphics(temp)
```

This report is part of the reproducibility review at the AGILE conference.
For more information see [https://reproducible-agile.github.io/](https://reproducible-agile.github.io/).
This document is published on OSF at https://osf.io/2sc7g/.
To cite the report use

> Nüst, D., & Graser, A. (2021, April 30). Reproducibility review of: Extraction of linear structures from digital terrain models using deep learning. <https://doi.org/10.17605/osf.io/2sc7g>

# Reviewed paper

> Satari, R., Kazimi, B., and Sester, M.: Extraction of linear structures from digital terrain models using deep learning, AGILE GIScience Ser., 2, 11, <https://doi.org/10.5194/agile-giss-2-11-2021>, 2021.

# Summary

The provided workflow was **partially reproduced**.
Based on the provided test file and instructions, I was able to recreate the computing environment and run the segmentation models.
Relevant tables from the paper could be recreated.
The training and validation part of the workflow is irreproducible because of proprietary data, therefore no figures could be recreated.

\clearpage

# Reproducibility reviewer notes

The first manuscript did not include a DASA section, but the authors provided a link to the project repository <https://github.com/Bashirkazimi/agile-submission-2021>.
The README file provides a step by step documentation.

```{bash env, eval=FALSE, size="tiny"}
git clone https://github.com/Bashirkazimi/agile-submission-2021

# first try using my local Python and GDAL, but Python 3.8 does not have tensorflow-gpu available in the required old version
gdalinfo --version
#GDAL 3.2.1, released 2020/12/29

# install python3.5, dev needed for GDAL
# sudo add-apt-repository ppa:deadsnakes/ppa
# sudo apt install python3.5 python3.5-dev

# in agile-submission-2021
mkvirtualenv agile-010 -p python3.5 -r requirements.txt
# following commands all in the venv

pip install GDAL==3.2.1
```

Next, I downloaded and unzipped the data per instructions from [here](https://seafile.cloud.uni-hannover.de/d/95a74b9a5b0e4e639077/), and tried to run the script.

```{bash create_dataset, eval=FALSE, size="tiny"}
python create_dataset.py
```

Leads to an error:

```txt
libcublas.so.9.0: cannot open shared object file: No such file or directory
```

So, let's check if I have a CUDA-compatible graphics card:

```{bash cuda, eval=FALSE, size="tiny"}
sudo lshw -C display

*-display                 
       description: VGA compatible controller
       product: UHD Graphics 620 (Whiskey Lake)
       vendor: Intel Corporation
       physical id: 2
       bus info: pci@0000:00:02.0
       version: 00
       width: 64 bits
       clock: 33MHz
       capabilities: pciexpress msi pm vga_controller bus_master cap_list rom
       configuration: driver=i915 latency=0
       resources: irq:150 memory:bb000000-bbffffff memory:60000000-7fffffff ioport:3000(size=64) memory:c0000-dffff
```

Problem: I don't have the right graphics card!
The authors said it should be possible with `tensorflow-cpu`, so I create `requirements-cpu.txt` with `tensorflow-cpu` version `1.15.0`, the only one for version `1.x`.

```{bash cpu, eval=FALSE, size="tiny"}
pip uninstall tensorflow_gpu
pip install -r requirements-cpu.txt
```

Need to make adjustments in `utils/utils.py` for my version of GDAL, later also in `HRNetBinary../evaluate.py`

```{python imports, eval=FALSE, size="tiny"}
# instead of import gdal, gdalconst
from osgeo import gdal, gdalconst

# instead of import ogr, osr
from osgeo import ogr, osr
```

```{bash createdataset_cpu, eval=FALSE, size="tiny"}
python create_dataset.py
# a lot of output...
```

Creates a lot of `.tif` files in `data/test`.

```{r tree, size="tiny"}
list.dirs(here::here("010/agile-submission-2021/data/"), full.names = FALSE)
```

## Evaluate segmentations

Next step in the instructions, binary segmentation wit HRNet:

```{bash hrnet, eval=FALSE, size="tiny"}
cd HRNetBinarySegmentation
python evaluate.py --evaluation_file=evaluation_file.csv
# [..]
Total params: 6,459,588
Trainable params: 6,446,018
Non-trainable params: 13,570
# [..]
```

Takes some time to run... then finishes with:

```{bash, eval=FALSE, size="tiny"}
[...]
192/192 [==============================] - 663s 3s/step
[0.2579103708461288, 0.9069677637591589, 0.7762661981036123, 0.8187425669110917, 0.7393777735160645]
```

And the file `evaluation_file.csv` is created as mentioned in the instructions.
The values in the file match the column `HRNet` of Table 1, within a level of precision to be expected from such a classification.

```{bash, eval=FALSE, size="tiny"}
# note the file name in the instructions has different capitalisation
cd SegnetBinarySegmentation
python3 evaluate.py --evaluation_file=evaluation_file.csv
```

This finished within a minute!
These values match the column `SegNet` of Table 1, within a level of precision to be expected from such a classification.

```{r table1, size="scriptsize", message=FALSE, caption = "Reproduction of Table 1 in the paper"}
hrnet <- read.csv("agile-submission-2021/HRNetBinarySegmentation/files/evaluation_file.csv")
segnet <- read.csv("agile-submission-2021/SegnetBinarySegmentation//files/evaluation_file.csv")

suppressPackageStartupMessages(library("tidyverse"))
dplyr::full_join(hrnet, segnet) %>%
  knitr::kable()
```

Run the next segmentation:

```{bash, eval=FALSE, size="tiny"}
cd multiclassSegmentation
python3 evaluate.py --evaluation_file=evaluation_file.csv
```

This completes and recreates the data in Table 3 within reasonable numerical precision based on the file `multiClassEvaluation.csv`.
It is unclear to me how Table 2 can be constructed from `evaluation_file.csv` of this segmentation, but I assume it can be.

```{r table2, size="scriptsize", message=FALSE, caption = "Reproduction of Table 3 in the paper"}
multi <- read.csv("agile-submission-2021/multiclassSegmentation/files/multiClassEvaluation.csv")

rows <- lapply(c(0:5), function(class) {
  classValues <- multi %>%
    dplyr::select(dplyr::ends_with(as.character(class)))
  names(classValues) <- c("sparse_iou", "prediction", "recall", "f1.score", "support")
  c(`Class label` = as.character(class), classValues)
})

dplyr::bind_rows(rows) %>%
  knitr::kable()
```

## Train and make predictions

In each experiment folder:

```{bash, eval=FALSE, size="tiny"}
python3 train_segmenter.py
python3 predict_sliding.py --output_shp_file=vector.shp --input_dtm=test_dtm.tif
```

```{bash, eval=FALSE, size="tiny"}
Leads to an error:
Traceback (most recent call last):
  File "train_segmenter.py", line 111, in <module>
    with open(os.path.join(train_dir, 'filenames.txt'), 'r') as reader:
FileNotFoundError: [Errno 2] No such file or directory: '../data/train/filenames.txt'
```

The path seems to be wrong, there only is  `filenames.txt` in `data/test`, but when using that, the code is missing the file `../data/valid/filenames.txt`.
A second view and short exchange with the authors made clear that the dataset is proprietary and belongs to a collaboration partner.

```{bash, echo=FALSE}
cd agile-submission-2021
git diff --ignore-all-space 0c1f128d..59524a71 > ../repro-review.diff
```

The diff of all changes made to the repository is stored in the file `repro-review.diff` published with this report and also in the fork [github.com/reproducible-agile/agile-submission-2021](https://github.com/reproducible-agile/agile-submission-2021).

---------

The workflow was partly reproduced. The data preparation (extraction with ArcGIS) is not reproducible, but the different segmentation algorithms could be executed.
The actual training of the models and prediction was not reproducible because of unavailable training and validation datasets.
The code seems to be complete tough and is readable, documented, and structured.
No code was provided to recreate any figures.

# Comments to the authors

A decent project, but there are always some things you can do better, here are some suggestions.

- Mostly a good job with the instructions in the README - please always test your own instructions - the file `test_labels.tif` was at the wrong location in the file share; ideally you store these file in a more long term location, for example by publishing your GitHub repo and the files in a Zenodo repository
- Suggest to provide a small script or CLI command to download the dataset, or mention to which directory the downloaded files should be extracted (`<project root>/data/dtms` or `<project root>/dtms`?)
- Note the code you "borrowed" from <https://github.com/niecongchong/HRNet-keras-semantic-segmentation> does not have a license attached to it, so I assume you asked the original author explicitly for permission to republish the code under the MIT license
- For machine learning workflows that use properitary data, please consider sharing a _synthetic dataset_ so that the full workflow can be executed by others
- Please provide more explicit documentation what part of the code reflects what part of the paper (mentioning chapters in the README, mentioning scripts in the paper)

```{r, echo=FALSE, eval=FALSE, results='hide'}
# create ZIP of reproduction files and upload to OSF
library("zip")
library("here")

zipfile <- here::here("010/reproreview-agile-2021-010.zip")
file.remove(zipfile)
zip::zipr(zipfile,
          here::here("010/repro-review.diff"))

library("osfr") # See docs at https://docs.ropensci.org/osfr/
# OSF_PAT is in .Renviron in parent directory
# We cannot use osfr to create a new component (with osfr::osf_create_component(x = osfr::osf_retrieve_node("6k5fh"), ...) because that will set the storage location to outside Europe.

# retrieve project
project <- osfr::osf_retrieve_node("2sc7g")

# upload files
osfr::osf_upload(x = project,
                 conflicts = "overwrite",
                 path = c(list.files(here::here("010"),
                                     pattern = "reproreview-agile-.*(pdf$|Rmd$|zip$)",
                                     full.names = TRUE)
                          )
                 )
```